# Data Engineer - Google Cloud Platform
<p align="center">Aprenda tudo sobre Engenharia de Dados, utilizando ferramentas mais atuais e muito mais.</p>

# Case: Indicador de Custo do Cr√©dito

Esse projeto visa trazer informa√ß√µes do [Banco Central do Brasil](https://dadosabertos.bcb.gov.br/), e realizar estudos em cima disso.

- [Indicador de Custo do Cr√©dito - ICC - Cr√©dito direcionado](https://dadosabertos.bcb.gov.br/dataset/25357-indicador-de-custo-do-credito---icc---credito-direcionado)

  Conceito: Custo m√©dio das opera√ß√µes de cr√©dito que integram a carteira de empr√©stimos, financiamentos e arrendamento mercantil das institui√ß√µes financeiras integrantes do Sistema Financeiro Nacional. Inclui todas as opera√ß√µes em aberto classificadas no ativo circulante, independente da data de contrata√ß√£o do cr√©dito.

 **Fonte: Banco Central do Brasil ‚Äì Departamento de Estat√≠sticas**

---
## Motiva√ß√£o

<p> A maior motiva√ß√£o para esse projeto √© poder deixar aqui tudo que eu sei e aprendi na √°rea de dados para outras pessoas, isso vai ajudar quem est√° aprendendo, e tamb√©m √© um lugar colaborativo. Onde voc√™ tamb√©m pode melhorar o que j√° tem.

Neste projeto estou utilizando o Google Colab, GCP para armazenar dados de Big Data e o DataBricks para visualizar e manipular via SQL.

---
## Pr√©-requisitos
Antes de come√ßar a utilizar deste projeto, confira se atende os seguintes requisitos:
- Tenha uma conta na Google e no DataBricks
- Saiba programar em `Python`
- No√ß√µes b√°sicas sobre `Apache Spark, GCP, Data Lake, DataBricks`

---
## Configurando Spark no Google Colab

### Instalar as depend√™ncias:
Aqui √© necess√°rio instalarmos o Java antes, porque o Spark utiliza do Java.

`!apt-get update -qq`

`!apt-get install openjdk-8-jdk-headless -qq > /dev/null`

`!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz`

`!tar xf spark-3.1.2-bin-hadoop2.7.tgz`

`!pip install -q findspark`

### Declarar vari√°veis de ambiente:
Para o nosso notebook funcionar da forma correta, temos que declarar o Java e Spark como vari√°vel de ambiente, assim ele n√£o vai dar erro ao executar por n√£o encontrar depend√™ncias.

`import os`

`os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"`

`os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop2.7"`

### Para encontrar o Spark

`import findspark`

`findspark.init()`

---
## Data Source - Drive

Nesse projeto, estou utilizando do Google Drive, para colocar o arquivo CSV em uma pasta e realizar a leitura.

Voc√™ pode baixar o arquivo CSV do Banco Central que est√° sendo utilizado, [clicando aqui](https://api.bcb.gov.br/dados/serie/bcdata.sgs.25357/dados?formato=csv).
Eu acabei renomeando o arquivo para `indicador_custo.csv`, mas fica ao seu cr√≠terio, s√≥ fique atento, porque o nome de arquivo pode estar diferente e isso ser um erro no seu c√≥digo.

Depois de ter criado uma pasta e colocado o arquivo.csv no seu drive, vamos importar ele para o nosso Google Colab.
Iremos em üóÇÔ∏è`Arquivos`, no seu canto superior esquerdo, e iremos na pasta do Drive, realizaremos a conex√£o, e √© importante depois disso ter em uma c√©lula o c√≥digo abaixo para funcionar.

`from google.colab import drive`

`drive.mount('/content/drive')`

---
## Criando uma Sess√£o Spark

`from pyspark.sql import SparkSession `

`spark = SparkSession.builder
  .master('local [*]') \
  .appName ("Iniciando com Spark") \
  .getorCreate ()`

Temos aqui um exemplo que podemos colocar o nome na nossa sess√£o, utilizando o appName()

O master com o local[*] serve para pegar todas as CPU‚Äôs dispon√≠veis, posso deixar vazio que ele vai entender ou ent√£o colocar uma quantidade, ele vai funcionar tamb√©m.

---
## Leitura CSV em Spark
O caminho relativo voc√™ pode copiar, clicando com o bot√£o direito no pr√≥prio arquivo.

`path = '/content/drive/MyDrive/data_sources/banco_central/credito_direcionado/indicador_custo.csv'`

`df = spark.read.csv(path, sep=';', inferSchema=True)`

---
## Transforma√ß√µes em Spark

Abrindo o arquivo do notebook no colab, voc√™ vai conseguir conferir todas as transforma√ß√µes desse projeto.
Incluindo:

- Select's
- Rename de colunas
- Count
- Transforma√ß√µes de dataTypes
- Transforma√ß√£o de data, exemplo: yyyy-mm-dd
- C√°lculos

---
### Refer√™ncias

[build-a-data-lake-on-gcp](https://cloud.google.com/architecture/build-a-data-lake-on-gcp?hl=pt-br#cloud-storage-as-data-lake)

---
## Autor

- **Anna Karoliny (@annakaroliny.tech)** - _Mentora, Desenvolvedora e Engenheira de Dados_
